{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import essential libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libs importées avec succès.\n"
     ]
    }
   ],
   "source": [
    "# Import joblib to save and load Python objects\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "\n",
    "# ML preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# ML models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# ML evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Libs importées avec succès.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL treatement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecté à la base de données : ../database/ELECTIONS.db\n"
     ]
    }
   ],
   "source": [
    "# --- 2. CONNEXION À LA BASE DE DONNÉES ---\n",
    "db_path = \"../database/ELECTIONS.db\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "print(f\"Connecté à la base de données : {db_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's agregate our datas per mandants (2017-2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données chargées et complétées par forward-fill.\n",
      "Shape total du DataFrame après remplissage : (752, 7)\n"
     ]
    }
   ],
   "source": [
    "# --- 3. CHARGEMENT ET REMPLISSAGE INTELLIGENT DES DONNÉES (FORWARD-FILL) ---\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    DEPARTMENT_CODE,\n",
    "    YEAR,\n",
    "    WINNER,\n",
    "    ROUND(AVG(POVERTY_RATE), 2) as avg_poverty_rate,\n",
    "    ROUND(AVG(UNEMPLOYMENT_RATE), 2) as avg_unemployment_rate,\n",
    "    ROUND(AVG(IMMIGRATION_RATE), 2) as avg_immigration_rate,\n",
    "    ROUND(AVG(NUMBER_OF_VICTIMS), 0) as avg_number_of_victims\n",
    "FROM ELECTIONS_ALL\n",
    "GROUP BY DEPARTMENT_CODE, YEAR\n",
    "ORDER BY DEPARTMENT_CODE, YEAR\n",
    "\"\"\"\n",
    "df_full = pd.read_sql(query, conn)\n",
    "\n",
    "# Propager le dernier gagnant connu pour chaque département\n",
    "df_full['WINNER'] = df_full.groupby('DEPARTMENT_CODE')['WINNER'].transform(lambda x: x.ffill())\n",
    "\n",
    "# Supprimer les lignes qui restent sans gagnant (si les toutes premières années n'ont pas de valeur)\n",
    "df_full.dropna(subset=['WINNER'], inplace=True)\n",
    "print(\"Données chargées et complétées par forward-fill.\")\n",
    "print(f\"Shape total du DataFrame après remplissage : {df_full.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our training datas (first and 2nd mandats)  \n",
    "We have to define our features (X) & target (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Taille du jeu d'entraînement (2017-2023) : (652, 7)\n",
      "Taille du jeu de test (2024) : (94, 7)\n"
     ]
    }
   ],
   "source": [
    "# --- 4. DIVISION TEMPORELLE STRICTE (TRAIN < 2024, TEST = 2024) ---\n",
    "train_df = df_full[df_full['YEAR'] < 2024].copy()\n",
    "test_df = df_full[df_full['YEAR'] == 2024].copy()\n",
    "\n",
    "# Par sécurité, on retire 'E.GAUCHE' qui a très peu d'échantillons et peut nuire à l'apprentissage\n",
    "train_df = train_df[train_df['WINNER'] != 'E.GAUCHE']\n",
    "\n",
    "print(f\"\\nTaille du jeu d'entraînement (2017-2023) : {train_df.shape}\")\n",
    "print(f\"Taille du jeu de test (2024) : {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features & Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jeux de données X_train/y_train et X_test/y_test créés.\n"
     ]
    }
   ],
   "source": [
    "# --- 5. DÉFINITION DES FEATURES (X) ET DE LA CIBLE (y) ---\n",
    "# Les features sont toutes les colonnes sauf 'WINNER' et 'YEAR'\n",
    "X_train = train_df.drop(columns=['WINNER', 'YEAR'])\n",
    "y_train = train_df['WINNER']\n",
    "X_test = test_df.drop(columns=['WINNER', 'YEAR'])\n",
    "y_test = test_df['WINNER']\n",
    "\n",
    "print(f\"\\nJeux de données X_train/y_train et X_test/y_test créés.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Désormais, nous devons identifier les features numériques & catégorielles pour les différencier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pipeline de pré-traitement et encodeur de label créés.\n"
     ]
    }
   ],
   "source": [
    "# --- 6. CRÉATION DU PIPELINE DE PRÉTRAITEMENT ---\n",
    "# Identification des colonnes\n",
    "categorical_features = ['DEPARTMENT_CODE']\n",
    "numerical_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "# Création du preprocessor pour les features (X)\n",
    "preprocessor_X = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Création de l'encodeur pour la cible (y)\n",
    "label_encoder_y = LabelEncoder()\n",
    "print(\"\\nPipeline de pré-traitement et encodeur de label créés.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformations appliquées aux données d'entraînement et de test.\n",
      "Shape de X_train_processed: (652, 98)\n",
      "Shape de X_test_processed: (94, 98)\n"
     ]
    }
   ],
   "source": [
    "# --- 7. APPLICATION DES TRANSFORMATIONS ---\n",
    "# On \"fit\" (apprend) sur le jeu d'entraînement et on transforme les deux jeux\n",
    "X_train_processed = preprocessor_X.fit_transform(X_train)\n",
    "X_test_processed = preprocessor_X.transform(X_test)\n",
    "\n",
    "y_train_encoded = label_encoder_y.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder_y.transform(y_test)\n",
    "print(\"\\nTransformations appliquées aux données d'entraînement et de test.\")\n",
    "print(f\"Shape de X_train_processed: {X_train_processed.shape}\")\n",
    "print(f\"Shape de X_test_processed: {X_test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 8. PRÉPARATION DES DONNÉES FINALES POUR LA SAUVEGARDE ---\n",
    "# Récupération des noms des colonnes après one-hot encoding\n",
    "try:\n",
    "    feature_names = preprocessor_X.get_feature_names_out()\n",
    "except AttributeError: # Fallback pour les anciennes versions de scikit-learn\n",
    "    ohe_feature_names = preprocessor_X.named_transformers_['cat']['onehot'].get_feature_names(input_features=categorical_features)\n",
    "    feature_names = numerical_features + list(ohe_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrames finaux prêts pour la sauvegarde.\n"
     ]
    }
   ],
   "source": [
    "# Création des DataFrames finaux\n",
    "X_train_processed_df = pd.DataFrame(X_train_processed.toarray() if hasattr(X_train_processed, \"toarray\") else X_train_processed, columns=feature_names)\n",
    "X_test_processed_df = pd.DataFrame(X_test_processed.toarray() if hasattr(X_test_processed, \"toarray\") else X_test_processed, columns=feature_names)\n",
    "\n",
    "# Ajout de la cible encodée pour la sauvegarde\n",
    "train_to_db = X_train_processed_df.copy()\n",
    "train_to_db['WINNER_encoded'] = y_train_encoded\n",
    "\n",
    "test_to_db = X_test_processed_df.copy()\n",
    "test_to_db['WINNER_encoded'] = y_test_encoded\n",
    "print(\"\\nDataFrames finaux prêts pour la sauvegarde.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Données sauvegardées dans les tables 'PROCESSED_TRAIN_DATA' ((652, 99)) et 'PROCESSED_TEST_DATA' ((94, 99)).\n"
     ]
    }
   ],
   "source": [
    "# --- 9. SAUVEGARDE DANS LA BASE DE DONNÉES ET DES TRANSFORMATEURS ---\n",
    "# Sauvegarde des DataFrames dans SQLite\n",
    "train_to_db.to_sql('PROCESSED_TRAIN_DATA', conn, if_exists='replace', index=False)\n",
    "test_to_db.to_sql('PROCESSED_TEST_DATA', conn, if_exists='replace', index=False)\n",
    "print(f\"\\nDonnées sauvegardées dans les tables 'PROCESSED_TRAIN_DATA' ({train_to_db.shape}) et 'PROCESSED_TEST_DATA' ({test_to_db.shape}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Préprocesseur sauvegardé dans : ../database/preprocessor_X.joblib\n",
      "Encodeur de label sauvegardé dans : ../database/label_encoder_y.joblib\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarde des transformateurs en fichiers .joblib\n",
    "db_dir = os.path.dirname(db_path)\n",
    "preprocessor_path = os.path.join(db_dir, 'preprocessor_X.joblib')\n",
    "label_encoder_path = os.path.join(db_dir, 'label_encoder_y.joblib')\n",
    "\n",
    "joblib.dump(preprocessor_X, preprocessor_path)\n",
    "joblib.dump(label_encoder_y, label_encoder_path)\n",
    "print(f\"Préprocesseur sauvegardé dans : {preprocessor_path}\")\n",
    "print(f\"Encodeur de label sauvegardé dans : {label_encoder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Connexion à la base de données fermée. Script terminé.\n"
     ]
    }
   ],
   "source": [
    "# --- 10. FERMETURE DE LA CONNEXION ---\n",
    "conn.close()\n",
    "print(\"\\nConnexion à la base de données fermée. Script terminé.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
