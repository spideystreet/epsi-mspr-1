{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prétraitement des Données pour la Modélisation Électorale\n",
    "\n",
    "Ce notebook a pour objectif de préparer les données brutes issues de la base de données `ELECTIONS.db`. Le processus inclut :\n",
    "1. Le chargement des données agrégées.\n",
    "2. Une division temporelle stricte pour créer un jeu d'entraînement et de test.\n",
    "3. La création d'un pipeline de prétraitement pour standardiser les données numériques et encoder les données catégorielles.\n",
    "4. La sauvegarde des données traitées et des transformateurs (pipelines) pour une utilisation future dans les notebooks d'entraînement et de prédiction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importation des Librairies\n",
    "\n",
    "Nous commençons par importer toutes les librairies Python nécessaires pour la manipulation de données, la connexion à la base de données, et le machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libs importées avec succès.\n"
     ]
    }
   ],
   "source": [
    "# Import joblib to save and load Python objects\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "\n",
    "# ML preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# ML models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# ML evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Libs importées avec succès.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Connexion à la Base de Données\n",
    "\n",
    "Nous établissons une connexion avec la base de données SQLite qui contient nos données électorales et socio-économiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecté à la base de données : ../database/ELECTIONS.db\n"
     ]
    }
   ],
   "source": [
    "# --- 2. CONNEXION À LA BASE DE DONNÉES ---\n",
    "db_path = \"../database/ELECTIONS.db\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "print(f\"Connecté à la base de données : {db_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chargement et Préparation des Données Brutes\n",
    "\n",
    "Nous exécutons une requête SQL pour agréger les données par département et par année. \n",
    "\n",
    "Ensuite, nous utilisons une stratégie de **forward-fill (`ffill`)**. Cela permet de propager le nom du parti gagnant des années précédentes aux années suivantes où l'information est manquante au sein d'un même département. C'est essentiel pour avoir une cible (`WINNER`) pour chaque ligne de donnée annuelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données chargées et complétées par forward-fill.\n",
      "Shape total du DataFrame après remplissage : (752, 7)\n"
     ]
    }
   ],
   "source": [
    "# --- 3. CHARGEMENT ET REMPLISSAGE INTELLIGENT DES DONNÉES (FORWARD-FILL) ---\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    DEPARTMENT_CODE,\n",
    "    YEAR,\n",
    "    WINNER,\n",
    "    ROUND(AVG(POVERTY_RATE), 2) as avg_poverty_rate,\n",
    "    ROUND(AVG(UNEMPLOYMENT_RATE), 2) as avg_unemployment_rate,\n",
    "    ROUND(AVG(IMMIGRATION_RATE), 2) as avg_immigration_rate,\n",
    "    ROUND(AVG(NUMBER_OF_VICTIMS), 0) as avg_number_of_victims\n",
    "FROM ELECTIONS_ALL\n",
    "GROUP BY DEPARTMENT_CODE, YEAR\n",
    "ORDER BY DEPARTMENT_CODE, YEAR\n",
    "\"\"\"\n",
    "df_full = pd.read_sql(query, conn)\n",
    "\n",
    "# Propager le dernier gagnant connu pour chaque département\n",
    "df_full['WINNER'] = df_full.groupby('DEPARTMENT_CODE')['WINNER'].transform(lambda x: x.ffill())\n",
    "\n",
    "# Supprimer les lignes qui restent sans gagnant (si les toutes premières années n'ont pas de valeur)\n",
    "df_full.dropna(subset=['WINNER'], inplace=True)\n",
    "print(\"Données chargées et complétées par forward-fill.\")\n",
    "print(f\"Shape total du DataFrame après remplissage : {df_full.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Division Temporelle : Jeu d'Entraînement et Jeu de Test\n",
    "\n",
    "Pour simuler une situation réelle où l'on prédit le futur à partir du passé, nous effectuons une **division temporelle stricte**. \n",
    "- Le jeu d'entraînement (`train_df`) contient toutes les données jusqu'à 2023.\n",
    "- Le jeu de test (`test_df`) ne contient que les données de 2024.\n",
    "\n",
    "Cela évite toute fuite de données du futur vers le passé. Nous en profitons également pour retirer la catégorie `E.GAUCHE` qui est très peu représentée et pourrait nuire à la qualité du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Taille du jeu d'entraînement (2017-2023) : (652, 7)\n",
      "Taille du jeu de test (2024) : (94, 7)\n"
     ]
    }
   ],
   "source": [
    "# --- 4. DIVISION TEMPORELLE STRICTE (TRAIN < 2024, TEST = 2024) ---\n",
    "train_df = df_full[df_full['YEAR'] < 2024].copy()\n",
    "test_df = df_full[df_full['YEAR'] == 2024].copy()\n",
    "\n",
    "# Par sécurité, on retire 'E.GAUCHE' qui a très peu d'échantillons et peut nuire à l'apprentissage\n",
    "train_df = train_df[train_df['WINNER'] != 'E.GAUCHE']\n",
    "\n",
    "print(f\"\\nTaille du jeu d'entraînement (2017-2023) : {train_df.shape}\")\n",
    "print(f\"Taille du jeu de test (2024) : {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Définition des Features (X) et de la Cible (y)\n",
    "\n",
    "Nous séparons nos jeux de données en variables explicatives (X) et en variable cible (y). Les colonnes `WINNER` et `YEAR` sont exclues des features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jeux de données X_train/y_train et X_test/y_test créés.\n"
     ]
    }
   ],
   "source": [
    "# --- 5. DÉFINITION DES FEATURES (X) ET DE LA CIBLE (y) ---\n",
    "# Les features sont toutes les colonnes sauf 'WINNER' et 'YEAR'\n",
    "X_train = train_df.drop(columns=['WINNER', 'YEAR'])\n",
    "y_train = train_df['WINNER']\n",
    "X_test = test_df.drop(columns=['WINNER', 'YEAR'])\n",
    "y_test = test_df['WINNER']\n",
    "\n",
    "print(f\"\\nJeux de données X_train/y_train et X_test/y_test créés.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Création du Pipeline de Pré-traitement\n",
    "\n",
    "Nous construisons un `ColumnTransformer` pour traiter différemment les types de colonnes :\n",
    "- **Variables Numériques** (`avg_poverty_rate`, etc.) : Seront standardisées (mises à l'échelle) avec `StandardScaler`.\n",
    "- **Variables Catégorielles** (`DEPARTMENT_CODE`) : Seront transformées en variables binaires avec `OneHotEncoder`.\n",
    "\n",
    "Parallèlement, nous créons un `LabelEncoder` pour convertir les noms des partis (texte) en chiffres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pipeline de pré-traitement et encodeur de label créés.\n"
     ]
    }
   ],
   "source": [
    "# --- 6. CRÉATION DU PIPELINE DE PRÉTRAITEMENT ---\n",
    "# Identification des colonnes\n",
    "categorical_features = ['DEPARTMENT_CODE']\n",
    "numerical_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "# Création du preprocessor pour les features (X)\n",
    "preprocessor_X = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Création de l'encodeur pour la cible (y)\n",
    "label_encoder_y = LabelEncoder()\n",
    "print(\"\\nPipeline de pré-traitement et encodeur de label créés.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Application des Transformations sur les Données\n",
    "\n",
    "C'est une étape cruciale. On **apprend** les paramètres de transformation (la moyenne pour la standardisation, la liste des catégories pour l'encodage, etc.) **uniquement sur le jeu d'entraînement** (avec `.fit_transform()`).\n",
    "\n",
    "Ensuite, on applique cette **même transformation**, sans ré-apprendre, sur le jeu de test (avec `.transform()`). Cela garantit que notre modèle ne \"voit\" aucune information du jeu de test pendant sa phase de préparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformations appliquées aux données d'entraînement et de test.\n",
      "Shape de X_train_processed: (652, 98)\n",
      "Shape de X_test_processed: (94, 98)\n"
     ]
    }
   ],
   "source": [
    "# --- 7. APPLICATION DES TRANSFORMATIONS ---\n",
    "# On \"fit\" (apprend) sur le jeu d'entraînement et on transforme les deux jeux\n",
    "X_train_processed = preprocessor_X.fit_transform(X_train)\n",
    "X_test_processed = preprocessor_X.transform(X_test)\n",
    "\n",
    "y_train_encoded = label_encoder_y.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder_y.transform(y_test)\n",
    "print(\"\\nTransformations appliquées aux données d'entraînement et de test.\")\n",
    "print(f\"Shape de X_train_processed: {X_train_processed.shape}\")\n",
    "print(f\"Shape de X_test_processed: {X_test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sauvegarde des Données Traitées et des Transformateurs\n",
    "\n",
    "Maintenant que nos données sont propres et prêtes pour la modélisation, nous les sauvegardons. Nous sauvegardons également les transformateurs pour pouvoir appliquer les mêmes transformations sur de nouvelles données à l'avenir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1. Reconstruction des DataFrames\n",
    "\n",
    "La sortie du pipeline est un tableau NumPy. Nous le convertissons en DataFrame pandas en récupérant les noms des colonnes pour une meilleure lisibilité et pour la sauvegarde en SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 8. PRÉPARATION DES DONNÉES FINALES POUR LA SAUVEGARDE ---\n",
    "# Récupération des noms des colonnes après one-hot encoding\n",
    "try:\n",
    "    feature_names = preprocessor_X.get_feature_names_out()\n",
    "except AttributeError: # Fallback pour les anciennes versions de scikit-learn\n",
    "    ohe_feature_names = preprocessor_X.named_transformers_['cat']['onehot'].get_feature_names(input_features=categorical_features)\n",
    "    feature_names = numerical_features + list(ohe_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrames finaux prêts pour la sauvegarde.\n"
     ]
    }
   ],
   "source": [
    "# Création des DataFrames finaux\n",
    "X_train_processed_df = pd.DataFrame(X_train_processed.toarray() if hasattr(X_train_processed, \"toarray\") else X_train_processed, columns=feature_names)\n",
    "X_test_processed_df = pd.DataFrame(X_test_processed.toarray() if hasattr(X_test_processed, \"toarray\") else X_test_processed, columns=feature_names)\n",
    "\n",
    "# Ajout de la cible encodée pour la sauvegarde\n",
    "train_to_db = X_train_processed_df.copy()\n",
    "train_to_db['WINNER_encoded'] = y_train_encoded\n",
    "\n",
    "test_to_db = X_test_processed_df.copy()\n",
    "test_to_db['WINNER_encoded'] = y_test_encoded\n",
    "print(\"\\nDataFrames finaux prêts pour la sauvegarde.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2. Sauvegarde dans la base de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Données sauvegardées dans les tables 'PROCESSED_TRAIN_DATA' ((652, 99)) et 'PROCESSED_TEST_DATA' ((94, 99)).\n"
     ]
    }
   ],
   "source": [
    "# --- 9. SAUVEGARDE DANS LA BASE DE DONNÉES ET DES TRANSFORMATEURS ---\n",
    "# Sauvegarde des DataFrames dans SQLite\n",
    "train_to_db.to_sql('PROCESSED_TRAIN_DATA', conn, if_exists='replace', index=False)\n",
    "test_to_db.to_sql('PROCESSED_TEST_DATA', conn, if_exists='replace', index=False)\n",
    "print(f\"\\nDonnées sauvegardées dans les tables 'PROCESSED_TRAIN_DATA' ({train_to_db.shape}) et 'PROCESSED_TEST_DATA' ({test_to_db.shape}).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3. Sauvegarde des transformateurs\n",
    "\n",
    "C'est l'étape la plus critique pour l'inférence future. En sauvegardant les objets `preprocessor_X` et `label_encoder_y` avec `joblib`, nous pourrons plus tard charger ces objets pour appliquer **exactement les mêmes transformations** sur de nouvelles données avant de faire une prédiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Préprocesseur sauvegardé dans : ../database/preprocessor_X.joblib\n",
      "Encodeur de label sauvegardé dans : ../database/label_encoder_y.joblib\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarde des transformateurs en fichiers .joblib\n",
    "db_dir = os.path.dirname(db_path)\n",
    "preprocessor_path = os.path.join(db_dir, 'preprocessor_X.joblib')\n",
    "label_encoder_path = os.path.join(db_dir, 'label_encoder_y.joblib')\n",
    "\n",
    "joblib.dump(preprocessor_X, preprocessor_path)\n",
    "joblib.dump(label_encoder_y, label_encoder_path)\n",
    "print(f\"Préprocesseur sauvegardé dans : {preprocessor_path}\")\n",
    "print(f\"Encodeur de label sauvegardé dans : {label_encoder_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Fermeture de la Connexion\n",
    "\n",
    "Finalement, nous fermons la connexion à la base de données pour libérer les ressources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Connexion à la base de données fermée. Script terminé.\n"
     ]
    }
   ],
   "source": [
    "# --- 10. FERMETURE DE LA CONNEXION ---\n",
    "conn.close()\n",
    "print(\"\\nConnexion à la base de données fermée. Script terminé.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
