{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import essential libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ANNEE DEPARTEMENT_CODE              DEPARTEMENT    WINNER  NB_INSCRITS  \\\n",
      "0   2024               01                      Ain  E.DROITE       446979   \n",
      "1   2024               02                    Aisne    DROITE        73181   \n",
      "2   2024               03                   Allier  E.DROITE       248529   \n",
      "3   2024               04  Alpes-de-Haute-Provence    GAUCHE       128146   \n",
      "4   2024               05             Hautes-Alpes    GAUCHE       114587   \n",
      "\n",
      "   NB_VOTANTS PARTI_1  VOIX_1   PARTI_2  VOIX_2  ... PARTI_5   VOIX_5 PARTI_6  \\\n",
      "0      311188  GAUCHE   19964    CENTRE  103368  ...  DROITE  27040.0     NaN   \n",
      "1       46620  DROITE   22933  E.DROITE   22409  ...     NaN      NaN     NaN   \n",
      "2      171908  GAUCHE   43029    DROITE   46601  ...     NaN      NaN     NaN   \n",
      "3       90407  GAUCHE   39040  E.DROITE   21536  ...     NaN      NaN     NaN   \n",
      "4       82882  GAUCHE   40743  E.DROITE   34857  ...     NaN      NaN     NaN   \n",
      "\n",
      "   VOIX_6 PARTI_7  VOIX_7 PARTI_8  VOIX_8 PARTI_9  VOIX_9  \n",
      "0     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "1     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "2     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "3     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "4     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "NaN values per column:\n",
      "ANNEE                 0\n",
      "DEPARTEMENT_CODE      0\n",
      "DEPARTEMENT           0\n",
      "WINNER                0\n",
      "NB_INSCRITS           0\n",
      "NB_VOTANTS            0\n",
      "PARTI_1               0\n",
      "VOIX_1                0\n",
      "PARTI_2               0\n",
      "VOIX_2                0\n",
      "PARTI_3              46\n",
      "VOIX_3               46\n",
      "PARTI_4             146\n",
      "VOIX_4              146\n",
      "PARTI_5             233\n",
      "VOIX_5              233\n",
      "PARTI_6             280\n",
      "VOIX_6              280\n",
      "PARTI_7             300\n",
      "VOIX_7              300\n",
      "PARTI_8             307\n",
      "VOIX_8              307\n",
      "PARTI_9             309\n",
      "VOIX_9              309\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ML preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# ML models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# ML evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the cleaned dataset\n",
    "df = pd.read_csv('../data/elections_prepared.csv')\n",
    "\n",
    "# Check the first rows\n",
    "print(df.head())\n",
    "\n",
    "# Check NaN values\n",
    "print(\"NaN values per column:\" )\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in WINNER column: ['E.DROITE' 'DROITE' 'GAUCHE' 'CENTRE' 'E.GAUCHE']\n",
      "Label mapping: {'CENTRE': np.int64(0), 'DROITE': np.int64(1), 'E.DROITE': np.int64(2), 'E.GAUCHE': np.int64(3), 'GAUCHE': np.int64(4)}\n",
      "Training set shape: (248, 23)\n",
      "Test set shape: (62, 23)\n",
      "Target distribution: \n",
      "WINNER_ENCODED\n",
      "0    136\n",
      "2     75\n",
      "4     59\n",
      "1     39\n",
      "3      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check unique values in the target variable\n",
    "print(\"Unique values in WINNER column:\", df['WINNER'].unique())\n",
    "\n",
    "# Encode the target variable \n",
    "label_encoder = LabelEncoder()\n",
    "df['WINNER_ENCODED'] = label_encoder.fit_transform(df['WINNER'])\n",
    "\n",
    "# Map to see the correspondence\n",
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(\"Label mapping:\", label_mapping)\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop(['WINNER', 'WINNER_ENCODED'], axis=1)\n",
    "y = df['WINNER_ENCODED']\n",
    "\n",
    "# Identify categorical and numerical features\n",
    "cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Create preprocessing pipeline with proper NaN handling\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='NONE')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_cols),\n",
    "        ('cat', categorical_transformer, cat_cols)\n",
    "    ])\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Target distribution: \\n{y.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression...\n",
      "Logistic Regression - Accuracy: 0.7419\n",
      "Training Random Forest...\n",
      "Random Forest - Accuracy: 0.6935\n",
      "Training Gradient Boosting...\n",
      "Gradient Boosting - Accuracy: 0.7258\n",
      "Training XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hich/Desktop/Bureau - MacbookAir HICHAM/PRO/GIT/epsi-mspr-1/venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [09:34:58] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost - Accuracy: 0.7419\n",
      "Training SVC...\n",
      "SVC - Accuracy: 0.7581\n",
      "\n",
      "Best model: SVC\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "columns are missing: {'DEPARTEMENT'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     47\u001b[39m X_predict = predict_2026.drop([\u001b[33m'\u001b[39m\u001b[33mWINNER\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mWINNER_ENCODED\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mDEPARTEMENT\u001b[39m\u001b[33m'\u001b[39m], axis=\u001b[32m1\u001b[39m)\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Make predictions (these will be encoded values)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m predictions_encoded = \u001b[43mbest_pipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_predict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Decode back to party names\u001b[39;00m\n\u001b[32m     53\u001b[39m predictions_2026 = label_encoder.inverse_transform(predictions_encoded)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Bureau - MacbookAir HICHAM/PRO/GIT/epsi-mspr-1/venv/lib/python3.13/site-packages/sklearn/pipeline.py:787\u001b[39m, in \u001b[36mPipeline.predict\u001b[39m\u001b[34m(self, X, **params)\u001b[39m\n\u001b[32m    785\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _routing_enabled():\n\u001b[32m    786\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iter(with_final=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m         Xt = \u001b[43mtransform\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    788\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m1\u001b[39m].predict(Xt, **params)\n\u001b[32m    790\u001b[39m \u001b[38;5;66;03m# metadata routing enabled\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Bureau - MacbookAir HICHAM/PRO/GIT/epsi-mspr-1/venv/lib/python3.13/site-packages/sklearn/utils/_set_output.py:319\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    322\u001b[39m         return_tuple = (\n\u001b[32m    323\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    324\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    325\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Bureau - MacbookAir HICHAM/PRO/GIT/epsi-mspr-1/venv/lib/python3.13/site-packages/sklearn/compose/_column_transformer.py:1090\u001b[39m, in \u001b[36mColumnTransformer.transform\u001b[39m\u001b[34m(self, X, **params)\u001b[39m\n\u001b[32m   1088\u001b[39m     diff = all_names - \u001b[38;5;28mset\u001b[39m(column_names)\n\u001b[32m   1089\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m diff:\n\u001b[32m-> \u001b[39m\u001b[32m1090\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcolumns are missing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdiff\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1091\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1092\u001b[39m     \u001b[38;5;66;03m# ndarray was used for fitting or transforming, thus we only\u001b[39;00m\n\u001b[32m   1093\u001b[39m     \u001b[38;5;66;03m# check that n_features_in_ is consistent\u001b[39;00m\n\u001b[32m   1094\u001b[39m     _check_n_features(\u001b[38;5;28mself\u001b[39m, X, reset=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mValueError\u001b[39m: columns are missing: {'DEPARTEMENT'}"
     ]
    }
   ],
   "source": [
    "# Define models to compare\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
    "    'SVC': SVC(probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "# Compare models using cross-validation\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'pipeline': pipeline\n",
    "    }\n",
    "    \n",
    "    print(f\"{name} - Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Find best model\n",
    "best_model_name = max(results.items(), key=lambda x: x[1]['accuracy'])[0]\n",
    "print(f\"\\nBest model: {best_model_name}\")\n",
    "\n",
    "# Tune the best model\n",
    "best_pipeline = results[best_model_name]['pipeline']\n",
    "\n",
    "# For future predictions, we'll need to decode the numeric predictions back to party names\n",
    "# So when we predict future elections:\n",
    "latest_year = df['ANNEE'].max()\n",
    "latest_data = df[df['ANNEE'] == latest_year].copy()\n",
    "\n",
    "predict_2026 = latest_data.copy()\n",
    "predict_2026['ANNEE'] = 2026\n",
    "\n",
    "X_predict = predict_2026.drop(['WINNER', 'WINNER_ENCODED', 'DEPARTEMENT'], axis=1)\n",
    "\n",
    "# Make predictions (these will be encoded values)\n",
    "predictions_encoded = best_pipeline.predict(X_predict)\n",
    "\n",
    "# Decode back to party names\n",
    "predictions_2026 = label_encoder.inverse_transform(predictions_encoded)\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    'DEPARTEMENT_CODE': predict_2026['DEPARTEMENT_CODE'],\n",
    "    'DEPARTEMENT': predict_2026['DEPARTEMENT'],\n",
    "    'WINNER_2024': latest_data['WINNER'],\n",
    "    'PREDICTED_WINNER_2026': predictions_2026\n",
    "})\n",
    "\n",
    "print(\"\\n2026 Election Predictions (Sample):\")\n",
    "print(results_df.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
